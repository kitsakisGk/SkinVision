{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 — Model Training\n",
    "\n",
    "In this notebook we:\n",
    "1. Load HAM10000 with our custom PyTorch Dataset\n",
    "2. Apply data augmentation (albumentations)\n",
    "3. Fine-tune **EfficientNet-B0** using transfer learning\n",
    "4. Handle class imbalance with weighted loss\n",
    "5. Train with early stopping and save the best model\n",
    "\n",
    "> Training on **CPU** — using 128x128 images and a lightweight model to keep it manageable.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')  # For interactive plots on Windows\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "from src.config import (\n",
    "    DATA_DIR, MODELS_DIR, RESULTS_DIR, SEED,\n",
    "    CLASS_NAMES, CLASS_LABELS, NUM_CLASSES,\n",
    "    IMAGE_SIZE, BATCH_SIZE, LEARNING_RATE, NUM_EPOCHS,\n",
    "    MODEL_NAME, EARLY_STOPPING_PATIENCE,\n",
    ")\n",
    "\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print(f'Device: {\"cuda\" if torch.cuda.is_available() else \"cpu\"}')\n",
    "print(f'Model: {MODEL_NAME}')\n",
    "print(f'Image size: {IMAGE_SIZE}x{IMAGE_SIZE}')\n",
    "print(f'Batch size: {BATCH_SIZE}')\n",
    "print(f'Epochs: {NUM_EPOCHS}')\n",
    "print(f'Learning rate: {LEARNING_RATE}')\n",
    "print()\n",
    "print('Setup complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Prepare the Data\n",
    "\n",
    "We'll use our custom `HAM10000Dataset` class with:\n",
    "- **Stratified splitting** — ensures all 7 classes appear in train/val/test\n",
    "- **Data augmentation** — flips, rotations, color jitter (train only)\n",
    "- **Weighted loss** — computed from class frequencies to handle imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset import prepare_dataloaders\n",
    "\n",
    "# Build image path map (images are split across two folders)\n",
    "# We need to consolidate the path for the dataset class\n",
    "image_dirs = [\n",
    "    DATA_DIR / 'HAM10000_images_part_1',\n",
    "    DATA_DIR / 'HAM10000_images_part_2',\n",
    "]\n",
    "\n",
    "# Load metadata and add the full image path\n",
    "metadata_path = DATA_DIR / 'HAM10000_metadata.csv'\n",
    "df = pd.read_csv(metadata_path)\n",
    "\n",
    "# Build path lookup\n",
    "image_path_map = {}\n",
    "for d in image_dirs:\n",
    "    for f in d.iterdir():\n",
    "        if f.suffix == '.jpg':\n",
    "            image_path_map[f.stem] = f\n",
    "\n",
    "print(f'Found {len(image_path_map)} images')\n",
    "print(f'Metadata rows: {len(df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from src.dataset import HAM10000Dataset, get_transforms\n",
    "\n",
    "# Stratified split: 70% train, 15% val, 15% test\n",
    "train_val_df, test_df = train_test_split(\n",
    "    df, test_size=0.15, stratify=df['dx'], random_state=SEED\n",
    ")\n",
    "train_df, val_df = train_test_split(\n",
    "    train_val_df, test_size=0.176, stratify=train_val_df['dx'], random_state=SEED\n",
    "    # 0.176 of 85% ≈ 15% of total\n",
    ")\n",
    "\n",
    "print(f'Train: {len(train_df)} ({len(train_df)/len(df)*100:.1f}%)')\n",
    "print(f'Val:   {len(val_df)} ({len(val_df)/len(df)*100:.1f}%)')\n",
    "print(f'Test:  {len(test_df)} ({len(test_df)/len(df)*100:.1f}%)')\n",
    "print()\n",
    "\n",
    "# Verify stratification\n",
    "print('Class distribution check (should be similar across splits):')\n",
    "for cls in CLASS_NAMES:\n",
    "    tr = (train_df['dx'] == cls).sum() / len(train_df) * 100\n",
    "    va = (val_df['dx'] == cls).sum() / len(val_df) * 100\n",
    "    te = (test_df['dx'] == cls).sum() / len(test_df) * 100\n",
    "    print(f'  {CLASS_LABELS[cls]:30s}  train:{tr:5.1f}%  val:{va:5.1f}%  test:{te:5.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create datasets with our custom class\n",
    "# We pass the image_path_map so it can find images across both folders\n",
    "train_dataset = HAM10000Dataset(train_df, image_dirs=image_dirs, transform=get_transforms('train'))\n",
    "val_dataset = HAM10000Dataset(val_df, image_dirs=image_dirs, transform=get_transforms('val'))\n",
    "test_dataset = HAM10000Dataset(test_df, image_dirs=image_dirs, transform=get_transforms('test'))\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "# Test a batch\n",
    "images, labels = next(iter(train_loader))\n",
    "print(f'Batch shape: {images.shape}')  # Should be [16, 3, 128, 128]\n",
    "print(f'Labels shape: {labels.shape}')\n",
    "print(f'Label values: {labels.tolist()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute class weights for imbalanced data\n",
    "label_counts = train_df['dx'].value_counts()\n",
    "total = len(train_df)\n",
    "class_weights = torch.tensor(\n",
    "    [total / (NUM_CLASSES * label_counts.get(c, 1)) for c in CLASS_NAMES],\n",
    "    dtype=torch.float32,\n",
    ")\n",
    "\n",
    "print('Class weights (higher = rarer class gets more attention):')\n",
    "for cls, w in zip(CLASS_NAMES, class_weights):\n",
    "    print(f'  {CLASS_LABELS[cls]:30s}: {w:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Augmented Samples\n",
    "\n",
    "Let's see what the augmentation does to the images — this is important to verify it's working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocessing import denormalize\n",
    "\n",
    "# Show a few augmented training samples\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "\n",
    "for i in range(10):\n",
    "    img, label = train_dataset[i]\n",
    "    img_display = denormalize(img)  # Undo normalization for display\n",
    "    ax = axes[i // 5, i % 5]\n",
    "    ax.imshow(img_display)\n",
    "    ax.set_title(CLASS_NAMES[label], fontsize=10)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Augmented Training Samples', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Build the Model\n",
    "\n",
    "We use **EfficientNet-B0** pretrained on ImageNet:\n",
    "1. First, **freeze** the base and only train the classifier head (fast)\n",
    "2. Then, **unfreeze** everything and fine-tune with a lower learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import create_model, freeze_base, unfreeze_all, get_model_summary\n",
    "\n",
    "# Create the model\n",
    "model = create_model(MODEL_NAME, NUM_CLASSES, pretrained=True)\n",
    "print(f'Model: {MODEL_NAME}')\n",
    "print(f'Output classes: {NUM_CLASSES}')\n",
    "print()\n",
    "\n",
    "# Freeze base layers — only train the classifier head first\n",
    "model = freeze_base(model)\n",
    "print('After freezing base:')\n",
    "get_model_summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Train — Phase 1: Classifier Head Only\n",
    "\n",
    "First we train just the classification head with the base frozen. This is fast and teaches the head to use the pretrained features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.train import train_model, set_seed\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "print('=== PHASE 1: Training classifier head (base frozen) ===')\n",
    "print('This should be fast even on CPU...\\n')\n",
    "\n",
    "model, history_phase1 = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    class_weights=class_weights,\n",
    "    num_epochs=5,           # Just a few epochs for the head\n",
    "    lr=1e-3,                # Higher LR since only training head\n",
    "    patience=3,\n",
    "    save_name='phase1_head.pth',\n",
    ")\n",
    "\n",
    "print('\\nPhase 1 complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Train — Phase 2: Full Fine-tuning\n",
    "\n",
    "Now we unfreeze everything and fine-tune the entire model with a lower learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze all layers\n",
    "model = unfreeze_all(model)\n",
    "print('After unfreezing all layers:')\n",
    "get_model_summary(model)\n",
    "\n",
    "print()\n",
    "print('=== PHASE 2: Full fine-tuning ===')\n",
    "print('This will take longer on CPU — grab a coffee...\\n')\n",
    "\n",
    "model, history_phase2 = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    class_weights=class_weights,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    lr=LEARNING_RATE,        # Lower LR for fine-tuning\n",
    "    patience=EARLY_STOPPING_PATIENCE,\n",
    "    save_name='best_model.pth',\n",
    ")\n",
    "\n",
    "print('\\nPhase 2 complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Training Curves\n",
    "\n",
    "Let's visualize how training went across both phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine histories from both phases\n",
    "history = {\n",
    "    'train_loss': history_phase1['train_loss'] + history_phase2['train_loss'],\n",
    "    'val_loss': history_phase1['val_loss'] + history_phase2['val_loss'],\n",
    "    'train_acc': history_phase1['train_acc'] + history_phase2['train_acc'],\n",
    "    'val_acc': history_phase1['val_acc'] + history_phase2['val_acc'],\n",
    "}\n",
    "\n",
    "from src.evaluate import plot_training_history\n",
    "\n",
    "plot_training_history(history, save=True)\n",
    "print(f'\\nBest val accuracy: {max(history[\"val_acc\"]):.4f}')\n",
    "print(f'Best val loss: {min(history[\"val_loss\"]):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Quick Test Set Evaluation\n",
    "\n",
    "Let's see how the trained model performs on the held-out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluate import get_predictions, print_classification_report\n",
    "\n",
    "device = torch.device('cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Get predictions on test set\n",
    "y_true, y_pred, y_probs = get_predictions(model, test_loader, device)\n",
    "\n",
    "print('=== TEST SET RESULTS ===')\n",
    "print()\n",
    "print_classification_report(y_true, y_pred)\n",
    "\n",
    "# Overall accuracy\n",
    "accuracy = (y_true == y_pred).mean()\n",
    "print(f'\\nOverall test accuracy: {accuracy:.4f} ({accuracy*100:.1f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluate import plot_confusion_matrix\n",
    "\n",
    "plot_confusion_matrix(y_true, y_pred, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save test predictions for later analysis\n",
    "import json\n",
    "\n",
    "test_results = {\n",
    "    'accuracy': float(accuracy),\n",
    "    'best_val_loss': float(min(history['val_loss'])),\n",
    "    'best_val_acc': float(max(history['val_acc'])),\n",
    "    'total_epochs': len(history['train_loss']),\n",
    "    'model': MODEL_NAME,\n",
    "    'image_size': IMAGE_SIZE,\n",
    "}\n",
    "\n",
    "with open(RESULTS_DIR / 'training_results.json', 'w') as f:\n",
    "    json.dump(test_results, f, indent=2)\n",
    "\n",
    "print('Results saved to results/training_results.json')\n",
    "print(json.dumps(test_results, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "What we did:\n",
    "1. Loaded 10,015 images with stratified 70/15/15 split\n",
    "2. Applied augmentation (flips, rotations, color jitter) to training data\n",
    "3. Used weighted cross-entropy loss to handle class imbalance\n",
    "4. **Phase 1**: Trained classifier head only (base frozen) — fast warm-up\n",
    "5. **Phase 2**: Fine-tuned entire model with lower learning rate\n",
    "6. Early stopping saved the best model by validation loss\n",
    "\n",
    "### Saved files:\n",
    "- `models/best_model.pth` — best model weights\n",
    "- `results/training_curves.png` — loss and accuracy plots\n",
    "- `results/confusion_matrix.png` — test set confusion matrix\n",
    "- `results/training_results.json` — metrics summary\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "-> **03_evaluation.ipynb** — deeper evaluation with ROC curves\n",
    "\n",
    "-> **04_gradcam.ipynb** — visualize what the model is looking at"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
