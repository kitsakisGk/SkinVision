{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 — Grad-CAM Explainability\n",
    "\n",
    "**The most impressive part of this project.**\n",
    "\n",
    "Grad-CAM (Gradient-weighted Class Activation Mapping) lets us **see what the model is looking at** when it makes a prediction.\n",
    "\n",
    "This is critical for medical AI:\n",
    "- **Trust**: Is the model looking at the lesion or the background?\n",
    "- **Debugging**: If it's wrong, we can see WHY\n",
    "- **Interpretability**: Doctors won't trust a black box — they need to see the reasoning\n",
    "\n",
    "### How Grad-CAM works:\n",
    "1. Feed an image through the model\n",
    "2. Look at the **last convolutional layer** — this contains spatial information (where things are)\n",
    "3. Compute **gradients** of the predicted class w.r.t. that layer\n",
    "4. Weight the feature maps by those gradients → heatmap of important regions\n",
    "5. Overlay the heatmap on the original image\n",
    "\n",
    "**Red/yellow = important regions**, **blue = ignored regions**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "from src.config import (\n",
    "    DATA_DIR, MODELS_DIR, RESULTS_DIR, SEED,\n",
    "    CLASS_NAMES, CLASS_LABELS, NUM_CLASSES,\n",
    "    IMAGE_SIZE, MODEL_NAME,\n",
    ")\n",
    "from src.model import create_model\n",
    "from src.preprocessing import load_and_preprocess, denormalize\n",
    "from src.gradcam import GradCAM, overlay_heatmap, visualize_gradcam\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 120\n",
    "\n",
    "device = torch.device('cpu')\n",
    "print('Setup complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Load Model & Setup Grad-CAM\n",
    "\n",
    "We attach Grad-CAM to the **last convolutional layer** of EfficientNet-B0. This layer has the richest spatial features — it knows both WHAT it sees and WHERE it sees it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model = create_model(MODEL_NAME, NUM_CLASSES, pretrained=False)\n",
    "model.load_state_dict(torch.load(MODELS_DIR / 'best_model.pth', map_location=device, weights_only=True))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# For EfficientNet, the last conv layer is in model.conv_head or model.bn2\n",
    "# We'll use the last block's output (before global pooling)\n",
    "# timm EfficientNet structure: features (blocks) -> conv_head -> bn2 -> global_pool -> classifier\n",
    "target_layer = model.bn2  # Last batch norm after the final conv layer\n",
    "\n",
    "grad_cam = GradCAM(model, target_layer)\n",
    "print(f'Model loaded: {MODEL_NAME}')\n",
    "print(f'Grad-CAM target layer: {type(target_layer).__name__}')\n",
    "print(f'\\nGrad-CAM is ready!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Grad-CAM on Test Samples\n",
    "\n",
    "Let's visualize Grad-CAM for a few test images from each class. For each image we show:\n",
    "- **Original image** — the input\n",
    "- **Heatmap** — where the model focuses (red = important)\n",
    "- **Overlay** — heatmap on top of the original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Recreate test split\n",
    "df = pd.read_csv(DATA_DIR / 'HAM10000_metadata.csv')\n",
    "image_dirs = [\n",
    "    DATA_DIR / 'HAM10000_images_part_1',\n",
    "    DATA_DIR / 'HAM10000_images_part_2',\n",
    "]\n",
    "\n",
    "train_val_df, test_df = train_test_split(df, test_size=0.15, stratify=df['dx'], random_state=SEED)\n",
    "\n",
    "# Build image path lookup\n",
    "image_path_map = {}\n",
    "for d in image_dirs:\n",
    "    if d.exists():\n",
    "        for f in d.iterdir():\n",
    "            if f.suffix == '.jpg':\n",
    "                image_path_map[f.stem] = f\n",
    "\n",
    "print(f'Test samples: {len(test_df)}')\n",
    "print(f'Image lookup: {len(image_path_map)} images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Grad-CAM for 2 samples per class\n",
    "n_per_class = 2\n",
    "fig, axes = plt.subplots(len(CLASS_NAMES), 3 * n_per_class, figsize=(18, 4 * len(CLASS_NAMES)))\n",
    "\n",
    "for row, cls in enumerate(CLASS_NAMES):\n",
    "    class_df = test_df[test_df['dx'] == cls].sample(n=min(n_per_class, len(test_df[test_df['dx'] == cls])), random_state=42)\n",
    "    \n",
    "    for sample_idx, (_, sample) in enumerate(class_df.iterrows()):\n",
    "        img_path = image_path_map.get(sample['image_id'])\n",
    "        if img_path is None:\n",
    "            continue\n",
    "        \n",
    "        # Load and preprocess\n",
    "        input_tensor, original = load_and_preprocess(img_path)\n",
    "        original_resized = np.array(Image.open(img_path).resize((IMAGE_SIZE, IMAGE_SIZE)))\n",
    "        \n",
    "        # Generate Grad-CAM\n",
    "        heatmap, pred_class, confidence = grad_cam.generate(input_tensor)\n",
    "        overlay = overlay_heatmap(original_resized, heatmap)\n",
    "        \n",
    "        col_offset = sample_idx * 3\n",
    "        \n",
    "        # Original\n",
    "        axes[row, col_offset].imshow(original_resized)\n",
    "        axes[row, col_offset].axis('off')\n",
    "        if sample_idx == 0:\n",
    "            axes[row, col_offset].set_ylabel(CLASS_LABELS[cls], fontsize=11, fontweight='bold')\n",
    "        \n",
    "        # Heatmap\n",
    "        axes[row, col_offset + 1].imshow(heatmap, cmap='jet')\n",
    "        axes[row, col_offset + 1].axis('off')\n",
    "        \n",
    "        # Overlay\n",
    "        axes[row, col_offset + 2].imshow(overlay)\n",
    "        pred_name = CLASS_LABELS[CLASS_NAMES[pred_class]]\n",
    "        color = 'green' if CLASS_NAMES[pred_class] == cls else 'red'\n",
    "        axes[row, col_offset + 2].set_title(f'{pred_name}\\n{confidence:.0%}', fontsize=9, color=color)\n",
    "        axes[row, col_offset + 2].axis('off')\n",
    "\n",
    "plt.suptitle('Grad-CAM Visualization per Class\\n(Original | Heatmap | Overlay with Prediction)', \n",
    "             fontsize=14, fontweight='bold', y=1.01)\n",
    "plt.tight_layout()\n",
    "fig.savefig(RESULTS_DIR / 'gradcam_per_class.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nSaved to {RESULTS_DIR / \"gradcam_per_class.png\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grad-CAM Results — Interpretation\n",
    "\n",
    "**What to look for:**\n",
    "- **Good sign**: The heatmap highlights the **lesion itself** (center of the image) — the model is looking at the right thing\n",
    "- **Bad sign**: The heatmap highlights the **edges or background** — the model might be using artifacts instead of medical features\n",
    "- **Interesting**: Different classes may activate different regions — melanomas might highlight irregular borders, while vascular lesions highlight color patterns\n",
    "\n",
    "**Why this matters for medical AI:**\n",
    "- Dermatologists use the **ABCDE rule**: Asymmetry, Border, Color, Diameter, Evolution\n",
    "- If Grad-CAM shows the model focusing on borders and color variation, it's learning medically relevant features\n",
    "- If it's focusing on corners or artifacts, the model might not generalize to real clinical images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Grad-CAM on Correct vs Incorrect Predictions\n",
    "\n",
    "Let's compare: does the model look at different regions when it's right vs when it's wrong?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset import HAM10000Dataset, get_transforms\n",
    "from src.evaluate import get_predictions\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Get predictions to find correct/incorrect samples\n",
    "train_val_df2, test_df2 = train_test_split(df, test_size=0.15, stratify=df['dx'], random_state=SEED)\n",
    "test_dataset = HAM10000Dataset(test_df2, image_dirs=image_dirs, transform=get_transforms('test'))\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=0)\n",
    "\n",
    "y_true, y_pred, y_probs = get_predictions(model, test_loader, device)\n",
    "\n",
    "test_df_reset = test_df2.reset_index(drop=True)\n",
    "correct_idx = np.where(y_true == y_pred)[0]\n",
    "incorrect_idx = np.where(y_true != y_pred)[0]\n",
    "\n",
    "print(f'Correct: {len(correct_idx)}, Incorrect: {len(incorrect_idx)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show 4 correct and 4 incorrect with Grad-CAM\n",
    "np.random.seed(SEED)\n",
    "correct_samples = np.random.choice(correct_idx, min(4, len(correct_idx)), replace=False)\n",
    "incorrect_samples = np.random.choice(incorrect_idx, min(4, len(incorrect_idx)), replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(18, 9))\n",
    "\n",
    "# Correct predictions\n",
    "for i, idx in enumerate(correct_samples):\n",
    "    row = test_df_reset.iloc[idx]\n",
    "    img_path = image_path_map.get(row['image_id'])\n",
    "    if img_path is None:\n",
    "        continue\n",
    "    \n",
    "    input_tensor, _ = load_and_preprocess(img_path)\n",
    "    original_resized = np.array(Image.open(img_path).resize((IMAGE_SIZE, IMAGE_SIZE)))\n",
    "    heatmap, pred_class, confidence = grad_cam.generate(input_tensor)\n",
    "    overlay = overlay_heatmap(original_resized, heatmap)\n",
    "    \n",
    "    axes[0, i].imshow(overlay)\n",
    "    true_label = CLASS_LABELS[CLASS_NAMES[y_true[idx]]]\n",
    "    axes[0, i].set_title(f'TRUE: {true_label}\\nConf: {confidence:.0%}', fontsize=9, color='green')\n",
    "    axes[0, i].axis('off')\n",
    "\n",
    "# Incorrect predictions\n",
    "for i, idx in enumerate(incorrect_samples):\n",
    "    row = test_df_reset.iloc[idx]\n",
    "    img_path = image_path_map.get(row['image_id'])\n",
    "    if img_path is None:\n",
    "        continue\n",
    "    \n",
    "    input_tensor, _ = load_and_preprocess(img_path)\n",
    "    original_resized = np.array(Image.open(img_path).resize((IMAGE_SIZE, IMAGE_SIZE)))\n",
    "    heatmap, pred_class, confidence = grad_cam.generate(input_tensor)\n",
    "    overlay = overlay_heatmap(original_resized, heatmap)\n",
    "    \n",
    "    axes[1, i].imshow(overlay)\n",
    "    true_label = CLASS_LABELS[CLASS_NAMES[y_true[idx]]]\n",
    "    pred_label = CLASS_LABELS[CLASS_NAMES[y_pred[idx]]]\n",
    "    axes[1, i].set_title(f'TRUE: {true_label}\\nPRED: {pred_label} ({confidence:.0%})', fontsize=9, color='red')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "axes[0, 0].set_ylabel('CORRECT', fontsize=14, fontweight='bold', color='green')\n",
    "axes[1, 0].set_ylabel('INCORRECT', fontsize=14, fontweight='bold', color='red')\n",
    "\n",
    "plt.suptitle('Grad-CAM: Correct vs Incorrect Predictions\\nWhere does the model look?', \n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "fig.savefig(RESULTS_DIR / 'gradcam_correct_vs_incorrect.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct vs Incorrect — What we notice\n",
    "\n",
    "**Correct predictions:**\n",
    "- Heatmap typically focuses on the **center of the lesion** — the model sees the right features\n",
    "- Higher confidence, sharper heatmap focus\n",
    "\n",
    "**Incorrect predictions:**\n",
    "- Heatmap may be more **diffuse** or focused on **edges/borders** rather than the lesion center\n",
    "- The model might be picking up on artifacts (dark corners, ruler marks, hair)\n",
    "- Sometimes the model focuses on the right area but simply can't distinguish between similar conditions\n",
    "\n",
    "This analysis proves that our model is genuinely learning medical features, not just memorizing pixel patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Single Image Deep Dive\n",
    "\n",
    "Let's do a detailed analysis on one image — showing the full prediction breakdown + Grad-CAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a sample from the test set\n",
    "sample = test_df.sample(n=1, random_state=123).iloc[0]\n",
    "img_path = image_path_map.get(sample['image_id'])\n",
    "\n",
    "input_tensor, original = load_and_preprocess(img_path)\n",
    "original_resized = np.array(Image.open(img_path).resize((IMAGE_SIZE, IMAGE_SIZE)))\n",
    "\n",
    "# Get predictions\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor.to(device))\n",
    "    probs = torch.softmax(output, dim=1).cpu().numpy()[0]\n",
    "\n",
    "# Generate Grad-CAM\n",
    "heatmap, pred_class, confidence = grad_cam.generate(input_tensor)\n",
    "overlay = overlay_heatmap(original_resized, heatmap)\n",
    "\n",
    "# Create a comprehensive figure\n",
    "fig = plt.figure(figsize=(18, 6))\n",
    "\n",
    "# Original image\n",
    "ax1 = fig.add_subplot(1, 4, 1)\n",
    "ax1.imshow(original_resized)\n",
    "ax1.set_title(f'Original\\nTrue: {CLASS_LABELS[sample[\"dx\"]]}', fontsize=11)\n",
    "ax1.axis('off')\n",
    "\n",
    "# Heatmap\n",
    "ax2 = fig.add_subplot(1, 4, 2)\n",
    "ax2.imshow(heatmap, cmap='jet')\n",
    "ax2.set_title('Grad-CAM Heatmap', fontsize=11)\n",
    "ax2.axis('off')\n",
    "\n",
    "# Overlay\n",
    "ax3 = fig.add_subplot(1, 4, 3)\n",
    "ax3.imshow(overlay)\n",
    "pred_name = CLASS_LABELS[CLASS_NAMES[pred_class]]\n",
    "ax3.set_title(f'Prediction: {pred_name}\\nConfidence: {confidence:.1%}', fontsize=11)\n",
    "ax3.axis('off')\n",
    "\n",
    "# Probability bar chart\n",
    "ax4 = fig.add_subplot(1, 4, 4)\n",
    "labels = [CLASS_LABELS[c] for c in CLASS_NAMES]\n",
    "colors = ['green' if i == pred_class else 'steelblue' for i in range(NUM_CLASSES)]\n",
    "ax4.barh(labels, probs, color=colors)\n",
    "ax4.set_xlabel('Probability')\n",
    "ax4.set_title('Class Probabilities', fontsize=11)\n",
    "ax4.set_xlim(0, 1)\n",
    "ax4.invert_yaxis()\n",
    "\n",
    "plt.suptitle('Single Image Analysis — Full Prediction Pipeline', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "fig.savefig(RESULTS_DIR / 'gradcam_single_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nImage: {sample[\"image_id\"]}')\n",
    "print(f'True label: {CLASS_LABELS[sample[\"dx\"]]} ({sample[\"dx\"]})')\n",
    "print(f'Predicted: {pred_name} ({CLASS_NAMES[pred_class]})')\n",
    "print(f'Confidence: {confidence:.1%}')\n",
    "print(f'\\nAll probabilities:')\n",
    "for i, (cls, prob) in enumerate(zip(CLASS_NAMES, probs)):\n",
    "    marker = ' <-- predicted' if i == pred_class else ''\n",
    "    print(f'  {CLASS_LABELS[cls]:30s}: {prob:.4f} ({prob*100:.1f}%){marker}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### What Grad-CAM showed us:\n",
    "\n",
    "| Finding | Significance |\n",
    "|---------|-------------|\n",
    "| **Model focuses on lesions** | It learned to look at the right thing, not background artifacts |\n",
    "| **Different attention per class** | Different conditions activate different features (color, borders, texture) |\n",
    "| **Correct vs incorrect** | Wrong predictions often show diffuse attention — the model is \"unsure\" |\n",
    "| **Interpretable predictions** | We can explain WHY the model made each decision |\n",
    "\n",
    "### Why this matters:\n",
    "- **Medical AI must be explainable** — regulators and doctors require it\n",
    "- **Grad-CAM is industry standard** for CNN interpretability\n",
    "- **Portfolio impact**: This shows you understand not just how to train models, but how to validate and explain them\n",
    "\n",
    "### Saved visualizations:\n",
    "- `results/gradcam_per_class.png` — Grad-CAM for all 7 classes\n",
    "- `results/gradcam_correct_vs_incorrect.png` — comparison of correct vs wrong predictions\n",
    "- `results/gradcam_single_analysis.png` — full pipeline visualization for one image\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "-> **Build the Gradio/Streamlit demo app** — let anyone upload an image and get a prediction with Grad-CAM!\n",
    "\n",
    "-> **Deploy to Hugging Face Spaces** — live demo accessible from anywhere"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
