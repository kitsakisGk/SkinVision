{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 — Deep Evaluation\n",
    "\n",
    "In the training notebook we got **75.8% test accuracy**. But accuracy alone doesn't tell the full story.\n",
    "\n",
    "In this notebook we dig deeper:\n",
    "1. **ROC Curves** — how well does the model separate each class from the rest?\n",
    "2. **Per-class analysis** — which conditions are hardest to classify?\n",
    "3. **Confidence analysis** — how confident is the model when it's right vs wrong?\n",
    "4. **Misclassified examples** — let's actually look at the images the model got wrong\n",
    "\n",
    "> This is the kind of analysis that separates a good data science project from a great one.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "from src.config import (\n",
    "    DATA_DIR, MODELS_DIR, RESULTS_DIR, SEED,\n",
    "    CLASS_NAMES, CLASS_LABELS, NUM_CLASSES,\n",
    "    IMAGE_SIZE, BATCH_SIZE, MODEL_NAME,\n",
    ")\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.dpi'] = 120\n",
    "\n",
    "device = torch.device('cpu')\n",
    "print(f'Device: {device}')\n",
    "print('Setup complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Load the Trained Model & Test Data\n",
    "\n",
    "We load the best model saved during training and recreate the same test set using the same random seed — this guarantees we evaluate on the exact same images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from src.dataset import HAM10000Dataset, get_transforms\n",
    "from src.model import create_model\n",
    "\n",
    "# Load metadata and recreate the same splits\n",
    "df = pd.read_csv(DATA_DIR / 'HAM10000_metadata.csv')\n",
    "image_dirs = [\n",
    "    DATA_DIR / 'HAM10000_images_part_1',\n",
    "    DATA_DIR / 'HAM10000_images_part_2',\n",
    "]\n",
    "\n",
    "# Same split as training (same seed = same split)\n",
    "train_val_df, test_df = train_test_split(df, test_size=0.15, stratify=df['dx'], random_state=SEED)\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=0.176, stratify=train_val_df['dx'], random_state=SEED)\n",
    "\n",
    "# Create test dataset and loader\n",
    "test_dataset = HAM10000Dataset(test_df, image_dirs=image_dirs, transform=get_transforms('test'))\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "# Load the best model\n",
    "model = create_model(MODEL_NAME, NUM_CLASSES, pretrained=False)\n",
    "model.load_state_dict(torch.load(MODELS_DIR / 'best_model.pth', map_location=device, weights_only=True))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f'Test samples: {len(test_df)}')\n",
    "print(f'Model loaded from: {MODELS_DIR / \"best_model.pth\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluate import get_predictions\n",
    "\n",
    "# Get all predictions on test set\n",
    "y_true, y_pred, y_probs = get_predictions(model, test_loader, device)\n",
    "\n",
    "accuracy = (y_true == y_pred).mean()\n",
    "print(f'Test accuracy: {accuracy:.4f} ({accuracy*100:.1f}%)')\n",
    "print(f'Total predictions: {len(y_true)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. ROC Curves (One-vs-Rest)\n",
    "\n",
    "**What is a ROC curve?**\n",
    "\n",
    "For each class, we ask: *\"How well can the model distinguish THIS class from ALL others?\"*\n",
    "\n",
    "- **X-axis (False Positive Rate)** — how often does it wrongly say \"yes\" when the answer is \"no\"?\n",
    "- **Y-axis (True Positive Rate / Recall)** — how often does it correctly say \"yes\" when the answer is \"yes\"?\n",
    "\n",
    "**AUC (Area Under Curve):**\n",
    "- **AUC = 1.0** — perfect classifier\n",
    "- **AUC = 0.5** — random guessing (the dashed diagonal line)\n",
    "- **AUC > 0.8** — good\n",
    "- **AUC > 0.9** — excellent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluate import plot_roc_curves\n",
    "\n",
    "plot_roc_curves(y_true, y_probs, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print AUC scores in a clean table\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "print('AUC Scores per class:')\n",
    "print('-' * 50)\n",
    "aucs = []\n",
    "for i, cls in enumerate(CLASS_NAMES):\n",
    "    binary_true = (y_true == i).astype(int)\n",
    "    auc_score = roc_auc_score(binary_true, y_probs[:, i])\n",
    "    aucs.append(auc_score)\n",
    "    label = CLASS_LABELS[cls]\n",
    "    bar = '█' * int(auc_score * 30)\n",
    "    print(f'  {label:30s}  AUC: {auc_score:.3f}  {bar}')\n",
    "\n",
    "print('-' * 50)\n",
    "print(f'  {\"Mean AUC\":30s}  AUC: {np.mean(aucs):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curves — Interpretation\n",
    "\n",
    "**What the AUC scores tell us:**\n",
    "- Classes with **AUC > 0.9** — the model can reliably distinguish these from other conditions\n",
    "- Classes with **AUC < 0.8** — the model struggles to separate these, likely because they look similar to other classes\n",
    "\n",
    "**Why ROC is better than accuracy for imbalanced data:**\n",
    "- Accuracy can be misleading (67% just by predicting \"mole\" for everything)\n",
    "- ROC/AUC evaluates performance at **all decision thresholds**, not just the default 0.5\n",
    "- In medical settings, you might want to adjust the threshold to catch more true positives (higher recall) at the cost of more false positives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Confidence Analysis\n",
    "\n",
    "Is the model **confident** when it's correct and **uncertain** when it's wrong?\n",
    "\n",
    "A well-calibrated model should:\n",
    "- Have **high confidence** for correct predictions\n",
    "- Have **lower confidence** for incorrect predictions\n",
    "\n",
    "If the model is equally confident when right and wrong, it can't be trusted — it doesn't \"know what it doesn't know.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the confidence (max probability) for each prediction\n",
    "confidences = y_probs.max(axis=1)\n",
    "correct_mask = y_true == y_pred\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Distribution of confidence for correct vs incorrect\n",
    "axes[0].hist(confidences[correct_mask], bins=30, alpha=0.7, label=f'Correct ({correct_mask.sum()})', color='green')\n",
    "axes[0].hist(confidences[~correct_mask], bins=30, alpha=0.7, label=f'Incorrect ({(~correct_mask).sum()})', color='red')\n",
    "axes[0].set_xlabel('Prediction Confidence')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Confidence Distribution: Correct vs Incorrect')\n",
    "axes[0].legend()\n",
    "\n",
    "# Average confidence per class\n",
    "class_conf_correct = []\n",
    "class_conf_incorrect = []\n",
    "for i, cls in enumerate(CLASS_NAMES):\n",
    "    mask = y_true == i\n",
    "    correct = mask & correct_mask\n",
    "    incorrect = mask & ~correct_mask\n",
    "    class_conf_correct.append(confidences[correct].mean() if correct.sum() > 0 else 0)\n",
    "    class_conf_incorrect.append(confidences[incorrect].mean() if incorrect.sum() > 0 else 0)\n",
    "\n",
    "x = np.arange(len(CLASS_NAMES))\n",
    "width = 0.35\n",
    "labels = [CLASS_LABELS[c] for c in CLASS_NAMES]\n",
    "axes[1].barh(x - width/2, class_conf_correct, width, label='Correct', color='green', alpha=0.7)\n",
    "axes[1].barh(x + width/2, class_conf_incorrect, width, label='Incorrect', color='red', alpha=0.7)\n",
    "axes[1].set_yticks(x)\n",
    "axes[1].set_yticklabels(labels)\n",
    "axes[1].set_xlabel('Mean Confidence')\n",
    "axes[1].set_title('Average Confidence by Class')\n",
    "axes[1].legend()\n",
    "axes[1].set_xlim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig(RESULTS_DIR / 'confidence_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nMean confidence (correct):   {confidences[correct_mask].mean():.3f}')\n",
    "print(f'Mean confidence (incorrect): {confidences[~correct_mask].mean():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence Analysis — Interpretation\n",
    "\n",
    "**What we want to see:**\n",
    "- The green bars (correct) should be **taller/more right** than the red bars (incorrect)\n",
    "- If the model is confident AND wrong, those are the most dangerous predictions\n",
    "\n",
    "**For medical AI, this matters a lot:**\n",
    "- A model that says \"I'm 95% sure this is benign\" when it's actually melanoma is dangerous\n",
    "- A model that says \"I'm 55% sure, maybe check with a doctor\" is much safer\n",
    "- This is why we might set a **confidence threshold** — only trust predictions above a certain confidence level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Per-Class Performance Deep Dive\n",
    "\n",
    "Let's visualize precision, recall, and F1 side by side for each class to see exactly where the model excels and struggles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Compute per-class metrics\n",
    "precisions = precision_score(y_true, y_pred, average=None, zero_division=0)\n",
    "recalls = recall_score(y_true, y_pred, average=None, zero_division=0)\n",
    "f1s = f1_score(y_true, y_pred, average=None, zero_division=0)\n",
    "\n",
    "# Create a DataFrame for easy viewing\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Class': [CLASS_LABELS[c] for c in CLASS_NAMES],\n",
    "    'Short': CLASS_NAMES,\n",
    "    'Precision': precisions,\n",
    "    'Recall': recalls,\n",
    "    'F1-Score': f1s,\n",
    "    'Support': [(y_true == i).sum() for i in range(NUM_CLASSES)],\n",
    "})\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = np.arange(len(CLASS_NAMES))\n",
    "width = 0.25\n",
    "\n",
    "bars1 = ax.bar(x - width, precisions, width, label='Precision', color='#4C72B0')\n",
    "bars2 = ax.bar(x, recalls, width, label='Recall', color='#DD8452')\n",
    "bars3 = ax.bar(x + width, f1s, width, label='F1-Score', color='#55A868')\n",
    "\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Per-Class Performance Metrics')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([CLASS_LABELS[c] for c in CLASS_NAMES], rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig(RESULTS_DIR / 'per_class_metrics.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(metrics_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Misclassified Examples\n",
    "\n",
    "Let's look at the actual images the model got wrong. This helps us understand:\n",
    "- Are the mistakes reasonable (ambiguous images that even doctors struggle with)?\n",
    "- Or is the model making obvious errors?\n",
    "\n",
    "We'll show the top misclassifications with the model's confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find misclassified samples\n",
    "test_df_reset = test_df.reset_index(drop=True)\n",
    "misclassified_idx = np.where(y_true != y_pred)[0]\n",
    "\n",
    "print(f'Total misclassified: {len(misclassified_idx)} / {len(y_true)} ({len(misclassified_idx)/len(y_true)*100:.1f}%)')\n",
    "print()\n",
    "\n",
    "# Build image path lookup\n",
    "image_path_map = {}\n",
    "for d in image_dirs:\n",
    "    if d.exists():\n",
    "        for f in d.iterdir():\n",
    "            if f.suffix == '.jpg':\n",
    "                image_path_map[f.stem] = f\n",
    "\n",
    "# Sort by confidence (highest confidence mistakes are most interesting)\n",
    "misclassified_conf = confidences[misclassified_idx]\n",
    "sorted_idx = misclassified_idx[np.argsort(-misclassified_conf)]  # Highest confidence first\n",
    "\n",
    "# Show top 12 most confident mistakes\n",
    "n_show = min(12, len(sorted_idx))\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "\n",
    "for i, idx in enumerate(sorted_idx[:n_show]):\n",
    "    row = test_df_reset.iloc[idx]\n",
    "    img_path = image_path_map.get(row['image_id'])\n",
    "    \n",
    "    ax = axes[i // 4, i % 4]\n",
    "    \n",
    "    if img_path and img_path.exists():\n",
    "        img = Image.open(img_path)\n",
    "        ax.imshow(img)\n",
    "    \n",
    "    true_label = CLASS_LABELS[CLASS_NAMES[y_true[idx]]]\n",
    "    pred_label = CLASS_LABELS[CLASS_NAMES[y_pred[idx]]]\n",
    "    conf = confidences[idx]\n",
    "    \n",
    "    ax.set_title(f'True: {true_label}\\nPred: {pred_label}\\nConf: {conf:.1%}', \n",
    "                 fontsize=8, color='red')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Most Confident Misclassifications (worst mistakes)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "fig.savefig(RESULTS_DIR / 'misclassified_examples.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misclassified Examples — What we learn\n",
    "\n",
    "**Looking at the images the model got wrong tells us a lot:**\n",
    "- Many misclassified images are genuinely ambiguous — they could reasonably be either class\n",
    "- Some errors are between visually similar classes (mel vs nv, akiec vs bkl)\n",
    "- The most dangerous mistakes are **high-confidence wrong predictions** — these are cases where the model is confidently incorrect\n",
    "\n",
    "**This kind of error analysis is what separates a portfolio project from a Kaggle submission.** Real-world ML engineers always examine their model's failures to understand limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Confusion Breakdown — Where exactly does the model confuse?\n",
    "\n",
    "Let's look at the most common error pairs to understand the model's systematic biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Find the biggest off-diagonal values (most common mistakes)\n",
    "errors = []\n",
    "for i in range(NUM_CLASSES):\n",
    "    for j in range(NUM_CLASSES):\n",
    "        if i != j and cm[i, j] > 0:\n",
    "            errors.append({\n",
    "                'true': CLASS_LABELS[CLASS_NAMES[i]],\n",
    "                'predicted': CLASS_LABELS[CLASS_NAMES[j]],\n",
    "                'count': cm[i, j],\n",
    "                'pct_of_true': cm[i, j] / cm[i].sum() * 100\n",
    "            })\n",
    "\n",
    "errors_df = pd.DataFrame(errors).sort_values('count', ascending=False).head(10)\n",
    "print('Top 10 Most Common Misclassifications:')\n",
    "print('=' * 70)\n",
    "for _, row in errors_df.iterrows():\n",
    "    print(f'  {row[\"true\"]:30s} -> {row[\"predicted\"]:30s}  ({int(row[\"count\"]):3d} cases, {row[\"pct_of_true\"]:.1f}% of actual)')\n",
    "\n",
    "print(f'\\nTotal errors: {(y_true != y_pred).sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Patterns — Interpretation\n",
    "\n",
    "**Common patterns in skin lesion classification errors:**\n",
    "- **mel → nv** (Melanoma predicted as Mole) — The most clinically dangerous error. Melanomas can look like regular moles, especially early-stage ones\n",
    "- **bkl → nv** (Benign Keratosis predicted as Mole) — Both are brownish, bumpy lesions\n",
    "- **nv → mel** (Mole predicted as Melanoma) — False alarm, but better safe than sorry in medicine!\n",
    "\n",
    "**Key insight:** Most errors are between visually similar classes. The model isn't making random mistakes — it's struggling with the same ambiguities that challenge dermatologists.\n",
    "\n",
    "**How to improve:**\n",
    "- Higher resolution images (224x224 or 384x384) would help catch subtle differences\n",
    "- More training data for rare classes\n",
    "- Ensemble of multiple models\n",
    "- Include patient metadata (age, sex, localization) as additional features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### What we learned from deep evaluation:\n",
    "\n",
    "| Analysis | Key Finding |\n",
    "|----------|-------------|\n",
    "| **ROC/AUC** | Model separates most classes well from each other |\n",
    "| **Confidence** | Model is more confident when correct — it \"knows what it knows\" |\n",
    "| **Per-class metrics** | Rare classes benefit from weighted loss, nv has highest precision |\n",
    "| **Misclassifications** | Most errors are between visually similar classes (mel↔nv, akiec↔bkl) |\n",
    "| **Error patterns** | Errors mirror real clinical challenges in dermatology |\n",
    "\n",
    "### Why this analysis matters:\n",
    "- **For the portfolio**: shows you don't just train models — you understand their behavior\n",
    "- **For real-world ML**: error analysis is essential before deploying any model\n",
    "- **For medical AI**: understanding failure modes is critical for patient safety\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "-> **04_gradcam.ipynb** — visualize WHERE the model looks when making predictions (explainability)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
